{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58439d47-e1b9-485c-9bd7-18a508e65d3d",
   "metadata": {},
   "source": [
    "### Comparing results from IsoQuant run on MASseq data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531b1da-ad90-417f-9536-402a656a6574",
   "metadata": {},
   "source": [
    "We first filter the output from IsoQuant to transcripts with at least 5 counts--this removes very rare transcripts which can be less reliable.\n",
    "\n",
    "After creating filtered GTF files, we compare them with the `gffcompare` tool (v0.12.6).\n",
    "\n",
    "```\n",
    "gffcompare -r [gencode_basic_gtf] [filtered_gtfs ...] --strict-match -o [gffcompare_path]/gffcmp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b951d-2f19-441f-b216-ff81160eedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import pickle\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysam\n",
    "\n",
    "from mdl.sc_isoform_paper import today\n",
    "from mdl.sc_isoform_paper.constants import MASSEQ_KEYS, MASSEQ_FILENAMES, SAMPLE_COLORS\n",
    "from mdl.sc_isoform_paper.plots import plot_dists\n",
    "from mdl.sc_isoform_paper.priming import PrimingClassifier\n",
    "from mdl.sc_isoform_paper.util import filter_gtf\n",
    "\n",
    "import upsetplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d196e70-320c-4e53-a7c0-75771c16faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the upsetplot package raises some deprecation warnings from pandas, but they can\n",
    "# be ignored for now\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "pysam.set_verbosity(0)\n",
    "\n",
    "root_dir = Path.home()\n",
    "sh_dir = root_dir / \"sh_scripts\"\n",
    "\n",
    "data_path = root_dir / \"data\" / \"masseq\"\n",
    "figure_path = root_dir / \"202501_figures\"\n",
    "\n",
    "# path to isoquant runs\n",
    "isoquant_path = data_path / \"20240722_isoquant\"\n",
    "# path to annotated BAMs\n",
    "annotated_path = data_path / \"20250124_annotated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78edc2-53c7-4298-84f8-080b73acdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gtf_path = data_path / f\"{today}_filtered_gtfs\"\n",
    "filtered_gtf_path.mkdir(exist_ok=True)\n",
    "\n",
    "for i in (1, 3, 4):\n",
    "    fn = MASSEQ_FILENAMES[i]\n",
    "    filter_gtf(\n",
    "        isoquant_path / fn / \"OUT\" /  \"OUT.transcript_models.gtf\",\n",
    "        filtered_gtf_path / f\"{fn}.gtf\",\n",
    "        isoquant_path / fn / \"OUT\" / \"OUT.transcript_model_counts.tsv\",\n",
    "    )\n",
    "\n",
    "gffcompare_path = data_path / f\"{today}_gffcompare\"\n",
    "gffcompare_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e071829-2ca8-462f-9e18-41e912c797ad",
   "metadata": {},
   "source": [
    "### GFFCompare\n",
    "\n",
    "After filtering the IsoQuant outputs to a minimum of 5 counts, we run `GffCompare` to match the results to each other and to the reference:\n",
    "\n",
    "```\n",
    "gffcompare -r GRCh38.gencode.v39.annotation.basic.gtf [filtered_gtf_path]/{pipseq_8x,10x_3p,10x_5p}.gtf --strict-match -o [gffcompare_path]/gffcmp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67cb80-ee55-4b0d-99d8-3b12ee06c2cf",
   "metadata": {},
   "source": [
    "### UpSet plots\n",
    "\n",
    "From here we can make the plots in Figure 2: UpSet plots showing the overlaps between the different samples, and violinplot showing the UMI distribution for unique and shared transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443789c5-c9d7-491a-a27c-591fbad074d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (\"gff_id\", \"xloc\", \"gene_tx\", \"class\")\n",
    "\n",
    "samples = tuple(MASSEQ_KEYS[i] for i in (1, 3, 4))\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73bff2-9073-4742-9db4-b205e42c2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gffcompare(gffcompare_tracking, cols):\n",
    "    \"\"\"read the tracking file and convert to a list of dictionaries for simpler access\"\"\"\n",
    "    with open(gffcompare_tracking) as fh:\n",
    "        rows = list(csv.DictReader(fh, fieldnames=cols, delimiter=\"\\t\"))\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def row_filter(rows, classes=None):\n",
    "    \"\"\"optionally filter gffcompare rows for a subset of classes\"\"\"\n",
    "    if classes is not None:\n",
    "        classes = set(classes)\n",
    "    yield from (r for r in rows if (classes is None or r[\"class\"] in classes))\n",
    "\n",
    "\n",
    "def make_overlap_df(rows, samples, classes=\"=\"):\n",
    "    sample_sets = defaultdict(list)\n",
    "    for i,r in enumerate(row_filter(rows, classes)):\n",
    "        for s in samples:\n",
    "            if r[s] != '-':\n",
    "                sample_sets[' '.join(s)].append(i)\n",
    "\n",
    "    return upsetplot.from_contents(sample_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939e85f-3b87-4e5d-bbad-1ccb4bdf71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = read_gffcompare(gffcompare_path / \"gffcmp.tracking\", cols + samples)\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc4d78-4282-4d27-beac-7dfa830be471",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "axs = upsetplot.plot(make_overlap_df(rows, samples, \"=c\"), sort_categories_by=\"input\", fig=fig, totals_plot_elements=0)\n",
    "fig.suptitle(\"GFFCompare Overlap, = and c\")\n",
    "plt.savefig(figure_path / \"fig2e_gffcompare_match.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0280e-a3c5-4c8b-8ddc-426fe882d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "all_overlap_df = make_overlap_df(rows, samples, None)\n",
    "axs = upsetplot.plot(all_overlap_df, sort_categories_by=\"input\", fig=fig, totals_plot_elements=0)\n",
    "fig.suptitle(\"GFFCompare Overlap, all tx\")\n",
    "plt.savefig(figure_path / \"fig2f_gffcompare_all.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a118d-6df3-40fa-a367-b67e534e58e4",
   "metadata": {},
   "source": [
    "## Counting reads and UMIs for shared and unique transcripts\n",
    "\n",
    "Here we collect two sets of counts: first, we break down the reads for each set of transcripts by their priming status, and compute the overall internal priming rate for unique vs shared transcripts.\n",
    "\n",
    "Next, we count the UMIs (as determined by the combination of transcript, sequence, and UMI) for each transcript in the two sets, and plot the distributions (Fig 2f).\n",
    "\n",
    "This takes a little while and requires the BAMs to count the UMIs. Instead we can load the results from a pickle and skip to **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23abff-5198-4f43-a0c7-1b2962d068c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file = data_path / \"gffcompare_stats.pickle\"\n",
    "if stats_file.exists():\n",
    "    with stats_file.open(\"rb\") as fh:\n",
    "        shared_read_priming, unique_read_priming, shared_counts, unique_counts = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5905a-d147-401f-b4b6-b4f2e1afc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sets = defaultdict(set)\n",
    "sample_tx_map = defaultdict(dict)\n",
    "\n",
    "for i, r in enumerate(row_filter(rows, None)):\n",
    "    for s in samples:\n",
    "        if r[s] != '-':\n",
    "            sample_sets[s].add(i)\n",
    "            sample_tx_map[s][i] = r[s].split(\"|\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25d809-c94c-4cc1-a31b-d02cef61c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_i = dict()\n",
    "shared_i = dict()\n",
    "\n",
    "for s in sample_sets:\n",
    "    unique_i[s] = sample_sets[s].difference(*(sample_sets[s2] for s2 in sample_sets if s != s2))\n",
    "    shared_i[s] = set.intersection(*sample_sets.values())\n",
    "\n",
    "u_sets = {s: {sample_tx_map[s][i] for i in unique_i[s]} for s in sample_sets}\n",
    "s_sets = {s: {sample_tx_map[s][i] for i in shared_i[s]} for s in sample_sets}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3d41b-28ad-475b-a369-f4b774feb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tags = frozenset(t.name for t in PrimingClassifier.GOOD_PRIMING_TAGS)\n",
    "\n",
    "def count_tx_priming(annotated_bam, tx_set, good_tags=good_tags):\n",
    "    tx_reads = defaultdict(Counter)\n",
    "    with pysam.AlignmentFile(annotated_bam, \"rb\", threads=2) as fh:\n",
    "        for a in fh:\n",
    "            if (tx := a.get_tag(\"YT\")) in tx_set:\n",
    "                tx_reads[tx][a.get_tag(\"XC\") in good_tags] += 1\n",
    "\n",
    "    return tx_reads\n",
    "\n",
    "def count_tx_umis(annotated_bam, tx_set):\n",
    "    tx_reads = defaultdict(set)\n",
    "    with pysam.AlignmentFile(annotated_bam, \"rb\", threads=2) as fh:\n",
    "        for a in fh:\n",
    "            if (tx := a.get_tag(\"YT\")) in tx_set:\n",
    "                tx_reads[tx].add((a.query, a.get_tag(\"UB\")))\n",
    "\n",
    "    return tx_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f9d19-8e0a-4584-8cd1-97d9f6592ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_read_priming = defaultdict(lambda: defaultdict(Counter))\n",
    "unique_read_priming = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "with ProcessPoolExecutor(8) as exc:\n",
    "    for i in [1, 3, 4]:\n",
    "        s = MASSEQ_KEYS[i]\n",
    "        anno_bams = sorted(annotated_path.glob(f\"*.{i}.*annotated.bam\"))\n",
    "        print(\" \".join(s), len(anno_bams))\n",
    "        for priming_c in exc.map(count_tx_priming, anno_bams, itertools.repeat(s_sets[s])):\n",
    "            for tx in priming_c:\n",
    "                shared_read_priming[s][tx] += priming_c[tx]\n",
    "        for priming_c in exc.map(count_tx_priming, anno_bams, itertools.repeat(u_sets[s])):\n",
    "            for tx in priming_c:\n",
    "                unique_read_priming[s][tx] += priming_c[tx]\n",
    "\n",
    "shared_read_priming = {s: dict(v) for s, v in shared_read_priming.items()}\n",
    "unique_read_priming = {s: dict(v) for s, v in unique_read_priming.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f1165-d8a3-4f9c-abf7-45e7e470fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_counts = dict()\n",
    "unique_counts = dict()\n",
    "\n",
    "with ProcessPoolExecutor(8) as exc:\n",
    "    for i in [1, 3, 4]:\n",
    "        s = MASSEQ_KEYS[i]\n",
    "        anno_bams = sorted(annotated_path.glob(f\"*.{i}.*annotated.bam\"))\n",
    "        print(\" \".join(s), len(anno_bams))\n",
    "\n",
    "        shared_reads = defaultdict(set)\n",
    "        for rld in exc.map(count_tx_umis, anno_bams, itertools.repeat(s_sets[s])):\n",
    "            for tx in rld:\n",
    "                shared_reads[tx].update(rld[tx])\n",
    "\n",
    "        shared_counts[s] = np.array([len(v) for v in shared_reads.values()])\n",
    "\n",
    "        unique_reads = defaultdict(set)\n",
    "        for rld in exc.map(count_tx_umis, anno_bams, itertools.repeat(u_sets[s])):\n",
    "            for tx in rld:\n",
    "                unique_reads[tx].update(rld[tx])\n",
    "\n",
    "        unique_counts[s] = np.array([len(v) for v in unique_reads.values()])\n",
    "\n",
    "# delete these to save memory\n",
    "del shared_reads, unique_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5448412-e4dc-4502-abef-488c8ae811f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stats_file.exists():\n",
    "    with stats_file.open(\"wb\") as out:\n",
    "        pickle.dump((shared_read_priming, unique_read_priming, shared_counts, unique_counts), out)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d4e89-4f0f-478f-b076-bf90f6864d06",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We print out some statistics and then make the plot for Figure 2g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b3974-befa-4a79-a2df-5228b257f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in samples:\n",
    "    print(s[0], np.percentile(shared_counts[s], (5, 50, 95)), np.percentile(unique_counts[s], (5, 50, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f5644-f1e3-492b-a475-5ead1e13b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in samples:\n",
    "    total_shared = sum(shared_read_priming[s].values(), start=Counter())\n",
    "    total_unique = sum(unique_read_priming[s].values(), start=Counter())\n",
    "    print(s[0])\n",
    "    print(f\"\\t{total_shared.total():,}\\t{total_shared[True] / total_shared.total():.1%}\\t\\t{len(s_sets[s]):,}\")\n",
    "    print(f\"\\t{total_unique.total():,}\\t{total_unique[True] / total_unique.total():.1%}\\t\\t{len(u_sets[s]):,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735aa54d-4051-4f36-9eac-766dd7b85f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "plot_dists(\n",
    "    ax, [cs[s] for cs in (unique_counts, shared_counts) for s in samples],\n",
    "    log=True,\n",
    "    colors=[SAMPLE_COLORS[s[0]] for s in samples * 2],\n",
    "    labels=[s[0] for s in samples * 2],\n",
    ")\n",
    "\n",
    "ax.set_yticks(\n",
    "    np.arange(7), [f\"$10^{i}$\" for i in range(7)], minor=False\n",
    ")\n",
    "ax.set_yticks(\n",
    "    np.log10([v*10**i for i in range(6) for v in range(2, 10)] + [2e6, 3e6]),\n",
    "    minor=True\n",
    ")\n",
    "ax.set_ylabel(\"UMIs per transcript\")\n",
    "\n",
    "plt.savefig(figure_path / \"fig2g_gffcompare_counts.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915e5d1-c791-4882-8683-5c25900a9038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa",
   "language": "python",
   "name": "fa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
