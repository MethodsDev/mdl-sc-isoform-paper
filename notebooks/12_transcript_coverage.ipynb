{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d309ac55-4910-4662-913c-3ce047ed5728",
   "metadata": {},
   "source": [
    "## Transcriptome coverage analysis\n",
    "\n",
    "In this notebook we're plotting the transcript coverage split four ways: good vs bad priming, and FSM vs ISM isoform matches. We start with the annotated bam files that we create from the internal priming analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca55ce-c374-4c19-9cc5-a0098547d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pysam\n",
    "\n",
    "\n",
    "import mdl.sc_isoform_paper.coverage as cov\n",
    "from mdl.sc_isoform_paper.constants import MASSEQ_KEYS, SAMPLE_COLORS\n",
    "from mdl.sc_isoform_paper.plots import plot_all_covs\n",
    "from mdl.sc_isoform_paper.priming import Priming, PrimingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf135879-72fc-47b2-ba1f-5571dd3be74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysam.set_verbosity(0)\n",
    "\n",
    "root_dir = Path.home()\n",
    "data_path = root_dir / \"data\" / \"masseq\"\n",
    "annotated_path = data_path / \"20250124_annotated\"\n",
    "figure_path = root_dir / \"202501_figures\"\n",
    "\n",
    "reference_path = root_dir / \"reference\"\n",
    "\n",
    "# generated with paftools.js\n",
    "gencode_basic_bed = reference_path /  \"GRCh38.gencode.v39.annotation.basic.bed\"\n",
    "tx_data = cov.TranscriptData(gencode_basic_bed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d731b-bf4e-4078-994e-2fc7debfbda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing coverage for PIPseq 0.8x SPRI, 10x 3', 10x 5'\n",
    "keys = [MASSEQ_KEYS[i] for i in (1, 3, 4)]\n",
    "annotated_bams = sorted(annotated_path.glob(\"*.[134].*annotated.bam\"))\n",
    "len(annotated_bams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f9ba3-bdce-411f-8a5b-de901bdae043",
   "metadata": {},
   "outputs": [],
   "source": [
    "priming_tags = tuple(p.name for p in Priming)\n",
    "splice_matches = (\"full_splice_match\", \"incomplete_splice_match\")\n",
    "\n",
    "good_tag_set = {p.name for p in PrimingClassifier.GOOD_PRIMING_TAGS}\n",
    "bad_tag_set = set(priming_tags) - good_tag_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db0442-de43-424b-b2df-b9b152153dc5",
   "metadata": {},
   "source": [
    "### Computing transcript coverage based on priming and splicing tags\n",
    "\n",
    "We need to go through the BAM files and partition the read coverage based on the priming and SQANTI classification tags. We parallelize the job over transcripts, which adds up to about 1.5M combinations: 24 BAMs x 61,314 transcripts. This takes a very long time (several hours) using `pysam`'s `count_coverage` method. The aggregated results (binned by transcript length) are available as `coverage_stats.pickle`. If this file is available, we can skip to **Results and Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a5293-99f7-4c6c-a8ae-31b49f628c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_stats_file = data_path / \"coverage_stats.pickle\"\n",
    "\n",
    "if coverage_stats_file.exists():\n",
    "    with coverage_stats_file.open(\"rb\") as fh:\n",
    "        tx_depth_bins, binned_tx = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc6128-a62a-452e-ab1e-50717f4501b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_tx_args = (\n",
    "    (MASSEQ_KEYS[int(anno_bam.name.split(\".\")[2])], anno_bam, tx, priming_tags, splice_matches)\n",
    "    for anno_bam in annotated_bams\n",
    "    for tx in tx_data\n",
    ")\n",
    "len(annotated_bams) * len(tx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b6304-4e67-4458-8e31-7796e472eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "per_tag_tx_depth = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "with ProcessPoolExecutor(16, initializer=cov.share_tx_data, initargs=(tx_data,)) as exc:\n",
    "    for tx, txd in exc.map(\n",
    "        cov.calc_cov_from_bam,\n",
    "        *zip(*per_tx_args),\n",
    "        chunksize=len(tx_data.loc)\n",
    "    ):\n",
    "        for ksc, arr in txd.items():\n",
    "            per_tag_tx_depth[ksc][tx] += arr\n",
    "\n",
    "per_tag_tx_depth = dict(per_tag_tx_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0fd5f-0e17-4273-84cb-d8117646b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate into good (expected) and bad (likely internally primed) categories\n",
    "\n",
    "per_tx_depth = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for k in keys:\n",
    "    for t in good_tag_set:\n",
    "        for s in splice_matches:\n",
    "            if (k,t,s) in per_tag_tx_depth:\n",
    "                for tx in per_tag_tx_depth[k,t,s]:\n",
    "                    per_tx_depth[k, \"good\", s][tx] += per_tag_tx_depth[k, t, s][tx]\n",
    "\n",
    "    for t in bad_tag_set:\n",
    "        for s in splice_matches:\n",
    "            if (k,t,s) in per_tag_tx_depth:\n",
    "                for tx in per_tag_tx_depth[k,t,s]:\n",
    "                    per_tx_depth[k, \"bad\", s][tx] += per_tag_tx_depth[k, t, s][tx]\n",
    "\n",
    "per_tx_depth = {k: dict(v) for k, v in per_tx_depth.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fe5d3-3ee5-4824-8352-bc6b23a649e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over three size bins: 0 - 2kb, 2kb - 4kb, and 4kb+\n",
    "tx_depth_bins, binned_tx = cov.overall_depth(tx_data, keys, per_tx_depth, [2000, 4000, 500000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849c08d-4dd3-4673-9c61-b77ff28f76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not coverage_stats_file.exists():\n",
    "    with coverage_stats_file.open(\"wb\") as out:\n",
    "        pickle.dump((tx_depth_bins, binned_tx), out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59eb61-2d6f-455a-8494-c0ca5328f354",
   "metadata": {},
   "source": [
    "## Results and Plotting\n",
    "\n",
    "First we'll print out some summary statistics for transcript coverage of incomplete splice matches. In \"good priming\" cases we see a strong enrichment for coverage at the 3' end of the transcript, while internal priming tends to lead to 5' enrichment for ISMs.\n",
    "\n",
    "Then, we plot the overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04576816-9498-49e9-aabc-ddc0a17d128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"length      \\ttechnology\")\n",
    "for k in sorted(keys):\n",
    "    for b1, b2 in itertools.pairwise([0, 2000, 4000, 500000]):\n",
    "        print(f\"{b1 // 1000}kb - {b2 // 1000}kb\", f\"{' '.join(k):10}\", sep=\"\\t\", end=\"\\t\")\n",
    "        tot = tx_depth_bins[b2][k, 'good', 'incomplete_splice_match'].sum()\n",
    "        print(\n",
    "            *(f\"{tx_depth_bins[b2][k, 'good', 'incomplete_splice_match'][i:j].sum() / tot:.1%}\"\n",
    "              for i, j in itertools.pairwise(np.linspace(0, 1000, 5, dtype=int))\n",
    "             ),\n",
    "            sep=\"\\t\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0aa3c-2239-49cd-b513-1f38f97ed7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"length      \\ttechnology\")\n",
    "for k in sorted(keys):\n",
    "    for b1, b2 in itertools.pairwise([0, 2000, 4000, 500000]):\n",
    "        print(f\"{b1 // 1000}kb - {b2 // 1000}kb\", f\"{' '.join(k):10}\", sep=\"\\t\", end=\"\\t\")\n",
    "        tot = tx_depth_bins[b2][k, 'bad', 'incomplete_splice_match'].sum()\n",
    "        print(\n",
    "            *(f\"{tx_depth_bins[b2][k, 'bad', 'incomplete_splice_match'][i:j].sum() / tot:.1%}\"\n",
    "              for i, j in itertools.pairwise(np.linspace(0, 1000, 5, dtype=int))\n",
    "             ),\n",
    "            sep=\"\\t\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03421859-c282-402e-8ae6-87a0f2696ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(4)\n",
    "bins = list(itertools.pairwise(np.linspace(0, 1000, x.shape[0] + 1, dtype=int)))\n",
    "p_cl_list = list(itertools.product((\"good\", \"bad\"), cov.SPLICE_MATCHES))\n",
    "fig, axs = plt.subplots(4, 3, figsize=(10, 8), sharey=\"row\", sharex=True)\n",
    "\n",
    "for ik, k in enumerate(sorted(keys)):\n",
    "    for j, (b1, b2) in enumerate(itertools.pairwise([0, 2000, 4000, 500000])):\n",
    "        axs[0, j].set_title(f\"{b1 // 1000}kb - {b2 // 1000}kb\")\n",
    "        for i, (p, cl) in enumerate(p_cl_list):\n",
    "            d = tx_depth_bins[b2][k, p, cl]\n",
    "            axs[i, j].bar(\n",
    "                x + 0.05 + ik * 0.3, [d[i:j].sum() / d.sum() for i,j in bins],\n",
    "                width=0.3, color=SAMPLE_COLORS[k[0]], align=\"edge\", label=\" \".join(k)\n",
    "            )\n",
    "            axs[i, j].axhline(1 / x.shape[0], color=\"k\", linestyle=\":\")\n",
    "\n",
    "            axs[i, j].set_xticks(x + 0.5)\n",
    "    \n",
    "    for i, (p, cl) in enumerate(p_cl_list):\n",
    "        axs[i, 0].set_ylabel(f\"{p}\\n{cl}\")\n",
    "\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"outside lower center\",\n",
    "    ncol=3,\n",
    ")\n",
    "\n",
    "plt.savefig(figure_path / \"supp_fig11_coverage_bins.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74755b88-fc55-467d-bf83-5e3e0ed0160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_covs(keys, tx_depth_bins, binned_tx, tx_data.last_exon_r, output_file=figure_path / \"fig2d_coverage.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d7bff-e8d3-4451-8906-5dd5d9d33146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "analysis",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "fa",
   "language": "python",
   "name": "fa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
